{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This notebook shows an example of making safety hazard identification based on sentence similarity and then output an excel table for evaluation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# read result from json file\n",
    "def read_result_from_json(json_path):\n",
    "\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "result = read_result_from_json('../processed_result.json')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# select images contain \"brick\" word in caption\n",
    "def select_image_with_word(result, words, number=100):\n",
    "    images = []\n",
    "    for img in tqdm(result):\n",
    "        if len(images) >= number:\n",
    "            break\n",
    "        # get all captions in image\n",
    "        captions = []\n",
    "        for group in img['bbox']:\n",
    "            for bbox in group:\n",
    "                captions.append(bbox['cap'])\n",
    "        # check if all the list of word in any one of captions\n",
    "        if all(word in ' '.join(captions) for word in words):\n",
    "            images.append(img)\n",
    "\n",
    "    return images\n",
    "\n",
    "brick_images = select_image_with_word(result, ['brick'])\n",
    "brick_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_similarity import *\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "nlp = spacy.load('en_use_md')\n",
    "\n",
    "rules = [\n",
    "    'worker should wear a safety helmet',\n",
    "    'worker should wear gloves'\n",
    "]\n",
    "# compare each group of caption in each image with a set of rules using semantic similarity\n",
    "def compare_caption_with_rules(images, rules, nlp, sim_threshold=0.6):\n",
    "    # parse the results into a pandas dataframe\n",
    "    df = pd.DataFrame(columns=['image_file', 'image', 'group', 'captions', 'rule', 'similarities', 'max_similarity', 'unsafe', 'rule_violated'])\n",
    "    for img in tqdm(images):\n",
    "        for g, group in enumerate(img['bbox']):\n",
    "            # get all captions in this group as a list of string\n",
    "            captions = [bbox['cap'] for bbox in group]\n",
    "            nlp_cap = spacy_parse_list(captions, nlp)\n",
    "            nlp_rule = spacy_parse_list(rules, nlp)\n",
    "            similarities = get_similarity_matrix(nlp_cap, nlp_rule)\n",
    "            similairty_list = similarities.max(axis=0).tolist()\n",
    "            # check if the max similarity is greater than the threshold and get the index of the rule\n",
    "            not_compliance = [sim < sim_threshold for sim in similairty_list]\n",
    "            index_violated = [i for i, x in enumerate(not_compliance) if x]\n",
    "            # get the rule violated\n",
    "            rule_violated = [rules[i] for i in index_violated]\n",
    "            # write to dataframe\n",
    "            df = df.append({\n",
    "                'image_file': img['img'],\n",
    "                # get only the file name\n",
    "                'image': os.path.basename(img['img']),\n",
    "                'group': g,\n",
    "                'captions': captions,\n",
    "                'rule': rules,\n",
    "                'similarities': similarities.tolist(),\n",
    "                'max_similarity': similairty_list,\n",
    "                'unsafe': len(index_violated)>0,\n",
    "                'rule_violated': rule_violated\n",
    "            }, ignore_index=True)\n",
    "            # embed the image file into the dataframe\n",
    "\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = compare_caption_with_rules(brick_images, rules, nlp, sim_threshold=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by image\n",
    "df.groupby('image_file').sum()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to excel file\n",
    "df.to_excel('height_images.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
